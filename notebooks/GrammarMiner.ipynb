{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Mining Input Grammars\n",
    "\n",
    "So far, the grammars we have seen have been mostly specified manually – that is, you (or the person knowing the input format) had to design and write a grammar in the first place.  While the grammars we have seen so far have been rather simple, creating a grammar for complex inoputs can involve quite some effort.  In this chapter, we therefore introduce techniques that automatically _mine_ grammars from programs – by executing the programs and observing how they process which parts of the input.  In conjunction with a grammar fuzzer, this allows us to (1) take a program, (2) extract its input grammar, and (3) fuzz it with high efficiency and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb).\n",
    "* The [chapter on configuration fuzzing](ConfigurationFuzzer.ipynb) introduces grammar mining for configuration options, as well as observing variables and values during execution.\n",
    "* The concept of parsing from [chapter on parsers](Parser.ipynb) is also useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the `parse_vehicle()` and `process_inventory()`  methods from the [chapter on parsers](Parser.ipynb) (modified to remove printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_van(year, company, model):\n",
    "    desc = \"We have a %s %s van from %s vintage.\" % (company, model, year)\n",
    "    iyear = int(year)\n",
    "    if iyear > 2010:\n",
    "        return \"%s\\nIt is a recent model!\" % desc\n",
    "    else:\n",
    "        return \"%s\\nIt is an old but reliable model!\" % desc\n",
    "\n",
    "\n",
    "def process_car(year, company, model):\n",
    "    desc = \"We have a %s %s car from %s vintage.\" % (company, model, year)\n",
    "    iyear = int(year)\n",
    "    if iyear > 2016:\n",
    "        return \"%s\\nIt is a recent model!\" % desc\n",
    "    else:\n",
    "        return \"%s\\nIt is an old but reliable model!\" % desc\n",
    "\n",
    "\n",
    "def process_vehicle(vehicle):\n",
    "    year, kind, company, model, *_ = vehicle.split(',')\n",
    "    if kind == 'van':\n",
    "        return process_van(year, company, model)\n",
    "    elif kind == 'car':\n",
    "        return process_car(year, company, model)\n",
    "    else:\n",
    "        raise Exception('Invalid entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process is driven by a method `process_inventory()` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inventory(inventory):\n",
    "    result = []\n",
    "    for vehicle in inventory.split('\\n'):\n",
    "        r = process_vehicle(vehicle)\n",
    "        result.append(r)\n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `process_inventory()` takes inputs of the following form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVENTORY = \"\"\"\\\n",
    "1997,van,Ford,E350\n",
    "2000,car,Mercury,Cougar\n",
    "1999,car,Chevy,Venture\\\n",
    "\"\"\"\n",
    "print(process_inventory(INVENTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found from the [chapter on parsers](Parser.ipynb) that coarse grammars do not work well for fuzzing when the input format includes details expressed only in code. That is, even though we have the formal specification of CSV files ([RFC 4180](https://tools.ietf.org/html/rfc4180)), the inventory system includes further rules as to what is expected at each index of the CSV file. The solution of simply recombining existing inputs, while practical, is incomplete. In particular, it relies on a formal input specification being available in the first place. However, we have no assurance that the program obeys the input specification given.\n",
    "\n",
    "One of the ways out of this predicament is to interrogate the program under test as to what its input specification is. That is, if the program under test is written in a recursive descent style, with specific methods responsible for handling specific parts of the input, one can recover the parse tree, by observing the process of parsing. Further, one can recover a reasonable approximation of the grammar by abstraction from multiple input trees.\n",
    "\n",
    "The idea is as follows\n",
    "* The assumption (1) is that the program is written in such a fashion that specific methods are responsible for parsing specific fragments of the program. This includes almost all ad hoc parsers.\n",
    "* We hook into the Python execution and observe how the input fragments are produced and named in different methods.\n",
    "* Stitch the input fragments together in a tree structure to produce a parse tree.\n",
    "* Abstract common elements from multiple parse trees to produce the Context Free Grammar of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## A Simple Grammar Miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to obtain the input grammar for the function `process_vehicle()`. We first collect the sample inputs for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES = INVENTORY.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen from the chapter on [configuration fuzzing](ConfigurationFuzzer.ipynb) that one can hook into the Python runtime to observe the arguments to a function and any local variables created. We have also seen that one can obtain the context of execution by inspecting the `frame` argument. Here is a simple tracer that can return the local variables and other contextual information in a traced function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceit(frame, event, arg):\n",
    "    method_name = frame.f_code.co_name\n",
    "    if method_name != \"process_vehicle\":\n",
    "        return\n",
    "    file_name = frame.f_code.co_filename\n",
    "    param_names = [frame.f_code.co_varnames[i] for i in range(frame.f_code.co_argcount)]\n",
    "    print(event, file_name, method_name, param_names, frame.f_locals)\n",
    "    return traceit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oldtrace = sys.gettrace()\n",
    "sys.settrace(traceit)\n",
    "process_vehicle(VEHICLES[0])\n",
    "sys.settrace(oldtrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We expand the `traceit()` tracer to a full fledged class `Tracer` that acts as a *context manager*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer:\n",
    "    def __enter__(self):\n",
    "        self.oldtrace = sys.gettrace()\n",
    "        sys.settrace(self.trace_event)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        sys.settrace(self.oldtrace)\n",
    "\n",
    "    def trace_event(self, frame, event, arg):\n",
    "        method_name = frame.f_code.co_name\n",
    "        if method_name != \"process_vehicle\":\n",
    "            return\n",
    "        file_name = frame.f_code.co_filename\n",
    "        param_names = [\n",
    "            frame.f_code.co_varnames[i]\n",
    "            for i in range(frame.f_code.co_argcount)\n",
    "        ]\n",
    "        print(event, file_name, method_name, param_names, frame.f_locals)\n",
    "        return self.trace_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is, any function executed under it gets a tracing hook installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer() as tracer:\n",
    "    process_vehicle(VEHICLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace produced by executing any function can get overwhelmingly large. Hence, we need restrict our attention to specific modules. Further, we also restrict our attention exclusively to `str` variables since these variables are more likely to contain input fragments. (We will show how to deal with complex objects later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a `Context` class that provides easy access to the information such as the current module, and parameter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, frame, track_caller=True):\n",
    "        self.method = self._method(frame)\n",
    "        self.parameter_names = self._get_parameters(frame)\n",
    "        self.file_name = self._file_name(frame)\n",
    "        self.parent = Context(frame.f_back,\n",
    "                              False) if track_caller and frame.f_back else None\n",
    "\n",
    "    def _get_parameters(self, frame):\n",
    "        return [\n",
    "            frame.f_code.co_varnames[i]\n",
    "            for i in range(frame.f_code.co_argcount)\n",
    "        ]\n",
    "\n",
    "    def _file_name(self, frame):\n",
    "        return frame.f_code.co_filename\n",
    "    \n",
    "    def _method(self, frame):\n",
    "        return frame.f_code.co_name\n",
    "\n",
    "    def all_vars(self, frame):\n",
    "        return frame.f_locals\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"%s:%s(%s)\" % (self.file_name, self.method, ','.join(self.parameter_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the context to decide which modules to monitor, and which variables to trace. Additionally, we store the current *input string* so that it can be used to determine if any particular string fragments came from the current input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def __init__(self, inputstr, files=[]):\n",
    "        self.inputstr, self.files, self.trace = inputstr, files, []\n",
    "\n",
    "    def tracing_var(self, k, v):\n",
    "        return isinstance(v, str)\n",
    "\n",
    "    def tracing_context(self, cxt, event, arg):\n",
    "        if not self.files:\n",
    "            return True\n",
    "        return any(cxt.file_name.endswith(f) for f in self.files)\n",
    "\n",
    "    def on_event(self, event, arg, cxt, my_vars):\n",
    "        self.trace.append((event, arg, cxt, my_vars))\n",
    "\n",
    "    def trace_event(self, frame, event, arg):\n",
    "        cxt = Context(frame)\n",
    "        if not self.tracing_context(cxt, event, arg):\n",
    "            return self.trace_event\n",
    "\n",
    "        my_vars = [(k, v) for k, v in cxt.all_vars(frame).items()\n",
    "                   if self.tracing_var(k, v)]\n",
    "        self.on_event(event, arg, cxt, my_vars)\n",
    "        return self.trace_event\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.inputstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Tracer` class can now focus on specific kinds of events on specific files. Further, it provides a first level filter for variables that we find interesting. For example, we want to focus specifically on `string` variables that contain input fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(VEHICLES[0]) as tracer:\n",
    "    process_vehicle(tracer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution produced the following trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer.trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `settrace()` function hooks into the Python debugging facility. When it is in operation, no debugger can hook into the program. Hence, we limit the tracer to the simplest implementation possible, and implement the core of grammar mining in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tracer simply records the variable values as they occur. We next need to check if the variables contain values from the **input string**. Common ways to do this is to rely on symbolic execution or at least dynamic tainting, which are powerful, but also complex. However, one can obtain a reasonable approximation by simply relying on substring search. That is, we consider any value produced that is a substring of the original input string to have come from the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of using substring search is that short string sequences tend to be included in other string sequences even though they may not have come from the original string. That is, say the input fragment is `v`. It could have equally come from either `van` or `chevy`. We rely on being able to predict the exact place input where a given fragment occurred. Hence, we define a constant `LOOKAHEAD_LEN` such that we ignore strings up to that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKAHEAD_LEN=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tracker identifies string fragments that are part of the input string, and stores them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    def __init__(self, inputstr, trace, **kwargs):\n",
    "        self.the_vars = {}\n",
    "        self.trace = trace\n",
    "        self.inputstr = inputstr\n",
    "        self.options(kwargs)\n",
    "        self.process()\n",
    "\n",
    "    def options(self, kwargs):\n",
    "        self.log = kwargs.get('log') or False\n",
    "\n",
    "    def include(self, var, value):\n",
    "        return len(value) > LOOKAHEAD_LEN and value in self.inputstr\n",
    "\n",
    "    def track_event(self, event, arg, cxt, my_vars):\n",
    "        self.the_vars.update({k: v for k, v in my_vars if self.include(k, v)})\n",
    "\n",
    "    def logger(self, var):\n",
    "        if self.log:\n",
    "            print(var)\n",
    "\n",
    "    def process(self):\n",
    "        for event, arg, cxt, my_vars in self.trace:\n",
    "            self.track_event(event, arg, cxt, my_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tracker, we can obtain the input fragments as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tracker = Tracker(tracer.inputstr, tracer.trace)\n",
    "for k,v in tracker.the_vars.items():\n",
    "    print(k, '=', repr(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Derivation Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input fragments from the `Tracker` can be used to obtain a derivation tree of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Grammars import START_SYMBOL, syntax_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation tree `Miner` is initialized with the input string, and the variable assignments, and it converts the assignments to the corresponding derivation tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner:\n",
    "    def __init__(self, my_input, my_assignments, **kwargs):\n",
    "        self.my_input = my_input\n",
    "        self.my_assignments = my_assignments\n",
    "        self.options(kwargs)\n",
    "        self.tree = self.get_derivation_tree()\n",
    "\n",
    "    def options(self, kwargs):\n",
    "        self.log = kwargs.get('log') or False\n",
    "\n",
    "    def logger(self, indent, var):\n",
    "        if self.log:\n",
    "            print('\\t' * indent, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is as follows:\n",
    "* We represent the derivation tree as a [straight line grammar](https://en.wikipedia.org/wiki/Straight-line_grammar) with each node represented by a key value pair. The key corresponds to the variable name, and the value corresponds to the representation of the value of the variable. The value representation may contain references to other nodes.\n",
    "* We start with a derivation tree with a single node -- the start symbol and the input string as its leaf.\n",
    "* For each pair _VAR_, _VALUE_ found in `my_assignments`:\n",
    "\n",
    "1. We search for occurrences of _VALUE_ in the grammar\n",
    "2. We replace them by <_VAR_>\n",
    "3. We add a new rule <_VAR_> $\\rightarrow$ <_VALUE_> to the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner(Miner):\n",
    "    def nonterminal(self, var):\n",
    "        return \"<\" + var.lower() + \">\"\n",
    "\n",
    "    def get_derivation_tree(self):\n",
    "        tree = {START_SYMBOL: (self.my_input, )}\n",
    "        my_assignments = self.my_assignments.copy()\n",
    "\n",
    "        while True:\n",
    "            new_rules = []\n",
    "            for var, value in my_assignments.items():\n",
    "                self.logger(0, \"%s = %s\" % (var, value))\n",
    "                for key, repl in tree.items():\n",
    "                    self.logger(1, \"%s : %s\" % (key, repl))\n",
    "                    if not any(value in t for t in repl):\n",
    "                        continue\n",
    "                    alt_key = self.nonterminal(var)\n",
    "                    new_arr = []\n",
    "                    for k, token in enumerate(repl):\n",
    "                        if not value in token:\n",
    "                            new_arr.append(token)\n",
    "                        else:\n",
    "                            arr = token.split(value)\n",
    "                            new_arr.extend(\n",
    "                                list(sum(zip(arr,\n",
    "                                             len(arr) * [alt_key]), ()))[:-1])\n",
    "                    tree[key] = tuple(i for i in new_arr if i)\n",
    "                    new_rules.append((var, alt_key, value))\n",
    "\n",
    "            if not new_rules:\n",
    "                break  # Nothing to expand anymore\n",
    "\n",
    "            for (var, alt_key, value) in new_rules:\n",
    "                tree[alt_key] = (value, )\n",
    "                self.logger(0, \"+%s = %s\" % (alt_key, value))\n",
    "\n",
    "                # Do not expand this again\n",
    "                del my_assignments[var]\n",
    "\n",
    "        return {key: values for key, values in tree.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Miner` is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(VEHICLES[0]) as tracer:\n",
    "    process_inventory(tracer())\n",
    "assignments = Tracker(tracer.inputstr, tracer.trace).the_vars\n",
    "dt = Miner(tracer.inputstr, assignments, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the grammar representation to a parse tree is accomplished by the `to_tree()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner(Miner):\n",
    "    def to_tree(self, key=START_SYMBOL):\n",
    "        if key not in self.tree:\n",
    "            return (key, [])\n",
    "        children = [self.to_tree(c) for c in self.tree[key]]\n",
    "        return (key, children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for VEHICLE in VEHICLES:\n",
    "    print(VEHICLE)\n",
    "    with Tracer(VEHICLE) as tracer:\n",
    "        process_inventory(tracer())\n",
    "    assignments = Tracker(tracer.inputstr, tracer.trace).the_vars\n",
    "    trees.append((tracer.inputstr, assignments))\n",
    "    for var, val in assignments.items():\n",
    "        print(var + \" = \" + repr(val))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarFuzzer import GrammarFuzzer, FasterGrammarFuzzer, display_tree, tree_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_dt = []\n",
    "for inputstr, assignments in trees:\n",
    "    print(inputstr)\n",
    "    dt = Miner(inputstr, assignments)\n",
    "    csv_dt.append(dt)\n",
    "    display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Recovering Grammar from Derivation Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class `Infer` that can combine multiple derivation trees to produce the grammar. The initial grammar is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infer:\n",
    "    def __init__(self):\n",
    "        self.grammar = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_tree()` method gets a combined list of non-terminals from current grammar, and the tree to be added to the grammar, and updates the definitions of each non-terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infer(Infer):\n",
    "    def add_tree(self, t):\n",
    "        merged_grammar = {}\n",
    "        for key in list(self.grammar.keys()) + list(t.tree.keys()):\n",
    "            alternates = set(self.grammar.get(key, []))\n",
    "            if key in t.tree:\n",
    "                alternates.add(''.join(t.tree[key]))\n",
    "            merged_grammar[key] = list(alternates)\n",
    "        self.grammar = merged_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Infer` is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Infer()\n",
    "for dt in csv_dt:\n",
    "    i.add_tree(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_diagram(i.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given execution traces from various inputs, one can define `recover_grammar()` to obtain the complete grammar from the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar(traces):\n",
    "    m = Infer()\n",
    "    for inputstr, trace in traces:\n",
    "        dt = Miner(inputstr, Tracker(inputstr, trace).the_vars)\n",
    "        m.add_tree(dt)\n",
    "    return m.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in VEHICLES:\n",
    "    with Tracer(inputstr) as tracer:\n",
    "        process_vehicle(tracer())\n",
    "    traces.append((tracer.inputstr, tracer.trace))\n",
    "grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = GrammarFuzzer(grammar)\n",
    "for i in range(10):\n",
    "    print(f.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2. Recovering URL Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = [\n",
    "    'http://user:pass@www.google.com:80/?q=path#ref',\n",
    "    'https://www.cispa.saarland:80/',\n",
    "    'http://www.fuzzingbook.org/#News',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, clear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in URLS:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, ['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    traces.append((tracer.inputstr, tracer.trace))\n",
    "grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_diagram(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS_X = URLS + ['ftp://freebsd.org/releases/5.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in URLS_X:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, ['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    traces.append((tracer.inputstr, tracer.trace))\n",
    "grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_diagram(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Miner with Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack(object):\n",
    "    def __init__(self, i):\n",
    "        self.original = i\n",
    "        self.inputs = []\n",
    "        \n",
    "    def height(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def has(self, val):\n",
    "        return any(val in var for var in self.inputs[-1].values())\n",
    "\n",
    "    def ignored(self, val):\n",
    "        return not (isinstance(val, str) and len(val) > LOOKAHEAD_LEN)\n",
    "\n",
    "    def include(self, k, val):\n",
    "        if self.ignored(val):\n",
    "            return False\n",
    "        return self.has(val) if self.inputs else val in self.original\n",
    "\n",
    "    def push(self, inputs):\n",
    "        my_inputs = {k: v for k, v in inputs.items() if self.include(k, v)}\n",
    "        self.inputs.append(my_inputs)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.inputs.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proxy the dictionary so that it will only update if it does not already contain a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vars(object):\n",
    "    def __init__(self, stack):\n",
    "        self.defs = {START_SYMBOL: stack.original}\n",
    "        self.istack = stack\n",
    "        \n",
    "    def set_kv(self, k, v):\n",
    "        if k not in self.defs:\n",
    "            self.defs[k] = v\n",
    "\n",
    "    def update(self, v):\n",
    "        for k,v in v.items():\n",
    "            self.set_kv(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackTracker(Tracker):\n",
    "    def __init__(self, inputstr, trace, **kwargs):\n",
    "        self.istack = InputStack(inputstr)\n",
    "        self.the_vars = Vars(self.istack)\n",
    "        self.trace = trace\n",
    "        self.options(kwargs)\n",
    "        self.process()\n",
    "\n",
    "    def options(self, kwargs):\n",
    "        self.track_params = kwargs.get('track_params') or True\n",
    "        self.track_vars = kwargs.get('track_vars') or True\n",
    "        self.track_return = kwargs.get('track_return') or False\n",
    "\n",
    "    def include(self, var, value):\n",
    "        return self.istack.include(var, value)\n",
    "\n",
    "    def get_params(self, cxt, all_vars):\n",
    "        return {\n",
    "            \"%s:%s\" % (cxt.method, k): v\n",
    "            for k, v in all_vars if k in cxt.parameter_names\n",
    "        }\n",
    "\n",
    "    def on_call(self, arg, cxt, my_vars):\n",
    "        my_parameters = {\n",
    "            k: v\n",
    "            for k, v in self.get_params(cxt, my_vars).items()\n",
    "            if not self.istack.ignored(v)\n",
    "        }\n",
    "        self.istack.push(my_parameters)\n",
    "        if self.track_params:\n",
    "            self.the_vars.update(my_parameters)\n",
    "\n",
    "    def on_line(self, arg, cxt, my_vars):\n",
    "        if self.track_vars:\n",
    "            qvars = {\"%s:%s\" % (cxt.method, k): v for k, v in my_vars}\n",
    "            my_vars = {\n",
    "                var: value\n",
    "                for var, value in qvars.items() if self.include(var, value)\n",
    "            }\n",
    "            if not self.track_params:\n",
    "                my_vars = {\n",
    "                    var: value\n",
    "                    for var, value in my_vas.items() if var not in param_names\n",
    "                }\n",
    "            self.the_vars.update(my_vars)\n",
    "\n",
    "    def on_return(self, arg, cxt, my_vars):\n",
    "        self.istack.pop()\n",
    "        self.on_line(arg, cxt, my_vars)\n",
    "        if self.track_return:\n",
    "            var = '(<-%s)' % cxt.method\n",
    "            if self.include(var, arg):\n",
    "                self.the_vars.update({var: arg})\n",
    "\n",
    "    def track_event(self, event, arg, cxt, my_vars):\n",
    "        if event == 'call':\n",
    "            return self.on_call(arg, cxt, my_vars)\n",
    "\n",
    "        if event == 'return':\n",
    "            return self.on_return(arg, cxt, my_vars)\n",
    "\n",
    "        if event == 'exception':\n",
    "            return\n",
    "\n",
    "        self.on_line(arg, cxt, my_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify `traceit()` to be aware of events now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url_traces = []\n",
    "for inputstr in URLS:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, ['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    sm = StackTracker(tracer.inputstr, tracer.trace)\n",
    "    url_traces.append((tracer.inputstr, sm))\n",
    "    for k,v in sm.the_vars.defs.items():\n",
    "        print(k, v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the following we do not account for parameters getting reassigned values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each (VAR, VALUE) found:\n",
    "* We search for occurrences of VALUE in the grammar\n",
    "* We replace them by VAR\n",
    "* We add a new rule VAR -> VALUE to the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner(Miner):\n",
    "    def get_derivation_tree(self):\n",
    "        my_assignments = self.my_assignments.copy()\n",
    "        tree = {}\n",
    "        for var, value in my_assignments.items():\n",
    "            nt_var = var if var == START_SYMBOL else self.nonterminal(var)\n",
    "            self.logger(0, \"%s = %s\" % (nt_var, value))\n",
    "            if tree:\n",
    "                append = False\n",
    "                for key, repl in tree.items():\n",
    "                    self.logger(1, \"%s : %s\" % (key, repl))\n",
    "                    if not any(value in t for t in repl):\n",
    "                        continue\n",
    "                    new_arr = []\n",
    "                    for k, token in enumerate(repl):\n",
    "                        if not value in token:\n",
    "                            new_arr.append(token)\n",
    "                        else:\n",
    "                            append = True\n",
    "                            arr = token.split(value)\n",
    "                            new_arr.extend(\n",
    "                                list(sum(zip(arr,\n",
    "                                             len(arr) * [nt_var]), ()))[:-1])\n",
    "                    tree[key] = tuple(i for i in new_arr if i)\n",
    "                if append:\n",
    "                    self.logger(0, \"+%s = %s\" % (nt_var, value))\n",
    "                    tree[nt_var] = set([value])\n",
    "            else:\n",
    "                tree[nt_var] = (value, )\n",
    "        return  {key: values for key, values in tree.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "with Tracer(URLS[2], ['urllib/parse.py']) as tracer:\n",
    "    urlparse(tracer())\n",
    "sm = StackTracker(tracer.inputstr, tracer.trace)\n",
    "dt = Miner(tracer.inputstr, sm.the_vars.defs)\n",
    "display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar(traces):\n",
    "    m = Infer()\n",
    "    for inputstr, trace in traces:\n",
    "        st = StackTracker(inputstr, trace)\n",
    "        dt = Miner(inputstr, st.the_vars.defs)\n",
    "        m.add_tree(dt)\n",
    "    return m.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in URLS_X:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, ['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    traces.append((tracer.inputstr, tracer.trace))\n",
    "grammar = recover_grammar(traces)\n",
    "syntax_diagram(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(INVENTORY) as tracer:\n",
    "    process_inventory(tracer())\n",
    "sm = StackTracker(tracer.inputstr, tracer.trace)\n",
    "for k,v in sm.the_vars.defs.items():\n",
    "    print(k,' = ',v)\n",
    "dt = Miner(tracer.inputstr, sm.the_vars.defs)\n",
    "display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tainted Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InformationFlow import tstr, ctstr, rewrite_in, Instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_METHOD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_method():\n",
    "    return tuple(CURRENT_METHOD)\n",
    "\n",
    "\n",
    "def set_current_method(method, method_stack):\n",
    "    global CURRENT_METHOD\n",
    "    CURRENT_METHOD = (method, len(method_stack), method_stack[-1])\n",
    "    return tuple(CURRENT_METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xtstr(ctstr):\n",
    "    def add_instr(self, op, c_a, c_b):\n",
    "        ct = None\n",
    "        if len(c_a) == 1 and isinstance(c_a, xtstr):\n",
    "            ct = c_a.taint[0]\n",
    "        elif len(c_b) == 1 and isinstance(c_b, xtstr):\n",
    "            ct = c_b.taint[0]\n",
    "        self.comparisons.append((ct, Instr(op, c_a, c_b), get_current_method()))\n",
    "\n",
    "    def create(self, res, taint):\n",
    "        o = xtstr(res, taint, self)\n",
    "        o.comparisons = self.comparisons\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we expand any object to a list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(key, val):\n",
    "    # Should we limit flatened objects to repr ~ tstr here or during call?\n",
    "    tv = type(val)\n",
    "    if isinstance(val, (int, float, complex, str, bytes, bytearray)):\n",
    "        return [(key, val)]\n",
    "    elif isinstance(val, (set, frozenset, list, tuple, range)):\n",
    "        values = [e for i, elt in enumerate(val) for e in flatten(i, elt)]\n",
    "        return [(\"%s.%d\" % (key, i), v) for i, v in values]\n",
    "    elif isinstance(val, dict):\n",
    "        values = [e for k, elt in val.items() for e in flatten(k, elt)]\n",
    "        return [(\"%s.%s\" % (key, k), v) for k, v in values]\n",
    "    elif isinstance(val, tstr):\n",
    "        return [(key, val)]\n",
    "    elif hasattr(val,'__dict__'):\n",
    "        values = [e for k, elt in val.__dict__.items() for e in flatten(k, elt)]\n",
    "        return [(\"%s.%s\" % (key, k), v) for k, v in values]\n",
    "    else:\n",
    "        return [(key, repr(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple miner, we do not need the input stack. All that we need is the ability to identify reassignments in variables. However, it makes life a little simpler if we can annotate variables with the stack depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedStack(InputStack):\n",
    "    # height has ignored include push pop\n",
    "    def __init__(self, i):  # same as original\n",
    "        self.original = i\n",
    "        self.inputs = []\n",
    "\n",
    "    # has is used only with include\n",
    "    def has(self, val):\n",
    "        assert False\n",
    "\n",
    "    def ignored(self, val):\n",
    "        return not (isinstance(repr(val), tstr))\n",
    "\n",
    "    # used from push (and Tracker)\n",
    "    def include(self, k, val):\n",
    "        if self.ignored(val):\n",
    "            return False\n",
    "        return val.taint_in(self.original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to account for custom data structures other than containers is to rely on its `repr()`. That is, both `str()` and `repr()` relies on string methods that we have overridden in the tainted string. Hence if any of the string fragments are tainted, their return will also tainted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedVars(Vars):\n",
    "    def __init__(self, stack):\n",
    "        self.accessed_seq_var = {}\n",
    "        self.taint_register = {}\n",
    "        self.method_id = None\n",
    "        super().__init__(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedVars(TaintedVars):\n",
    "    def update(self, values):\n",
    "        vals = [(k1, v1) for k, v in values.items() for k1, v1 in flatten(k, v)]\n",
    "        for k, v in vals:\n",
    "            self.set_kv(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_METHOD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedVars(TaintedVars):\n",
    "    def set_current_method(self, method):\n",
    "        self.method_id = method\n",
    "\n",
    "    def var_init(self, var):\n",
    "        if var not in self.accessed_seq_var:\n",
    "            self.accessed_seq_var[var] = 0\n",
    "\n",
    "    def var_assign(self, var):\n",
    "        self.accessed_seq_var[var] += 1\n",
    "\n",
    "    def var_name(self, var):\n",
    "        var_seq = self.accessed_seq_var[var]\n",
    "        # TODO: figure out how to deal with stack/vars stack height\n",
    "        method, stack, method_seq = self.method_id\n",
    "        return \"%s[stack:%d mseq:%d vseq:%d]\" % (var, stack, method_seq, var_seq)\n",
    "\n",
    "    def set_kv(self, var, val):\n",
    "        self.var_init(var)\n",
    "        sa_var = self.var_name(var)\n",
    "        if sa_var not in self.defs:\n",
    "            self.defs[sa_var] = val\n",
    "            self.taint_register[str(\n",
    "                val.taint)] = self.taint_register.get(str(val.taint)) or set()\n",
    "            self.taint_register[str(val.taint)].add(sa_var)\n",
    "        else:  # possible reassignment\n",
    "            tainted_vars = self.taint_register.get(str(val.taint))\n",
    "            if tainted_vars is None or sa_var not in tainted_vars:  # a change in taint\n",
    "                self.var_assign(var)\n",
    "                sa_var = self.var_name(var)\n",
    "                self.defs[sa_var] = val\n",
    "                self.taint_register[str(val.taint)] = self.taint_register.get(\n",
    "                    str(val.taint)) or set()\n",
    "                self.taint_register[str(val.taint)].add(sa_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedTracer(Tracer):\n",
    "    def __init__(self, inputstr, restrict={}):\n",
    "        self.inputstr = xtstr(inputstr, parent=None).with_comparisons([])\n",
    "        self.trace = []\n",
    "        self.restrict = restrict\n",
    "\n",
    "        self.method_num = 0\n",
    "        self.method_num_stack = [self.method_num]\n",
    "\n",
    "    def tracing_var(self, k, v):\n",
    "        return isinstance(repr(v), tstr)\n",
    "\n",
    "    def tracing_context(self, cxt, event, arg):\n",
    "        if self.restrict.get('files'):\n",
    "            return any(\n",
    "                cxt.file_name.endswith(f) for f in self.restrict['files'])\n",
    "        if self.restrict.get('methods'):\n",
    "            return cxt.method in self.restrict['methods']\n",
    "        return True\n",
    "\n",
    "    def on_event(self, event, arg, cxt, my_vars):\n",
    "        super().on_event(event, arg, cxt, my_vars)\n",
    "        if event == 'call':\n",
    "            self.method_num += 1\n",
    "            self.method_num_stack.append(self.method_num)\n",
    "        elif event == 'return':\n",
    "            self.method_num_stack.pop()\n",
    "        set_current_method(cxt.method, self.method_num_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedTracker(StackTracker):\n",
    "    def __init__(self, inputstr, trace, **kwargs):\n",
    "        self.istack = TaintedStack(inputstr)\n",
    "        self.inputstr = inputstr\n",
    "        self.the_vars = TaintedVars(self.istack)\n",
    "        self.trace = trace\n",
    "        self.mstack = ['']\n",
    "        self.method_register = {}\n",
    "\n",
    "\n",
    "        self.options(kwargs)\n",
    "        self.process()\n",
    "\n",
    "    def call_enter(self, method):\n",
    "        key = (method, len(self.mstack))\n",
    "        if key not in self.method_register:\n",
    "            self.method_register[key] = 0\n",
    "        else:\n",
    "            self.method_register[key] += 1\n",
    "        method_id = (*key, self.method_register[key])\n",
    "        print(\"->\", \"\\t\" * len(self.mstack), method_id)\n",
    "        self.mstack.append(method_id)\n",
    "        self.the_vars.set_current_method(self.mstack[-1])\n",
    "\n",
    "    def call_exit(self, method):\n",
    "        m = self.mstack.pop()\n",
    "        print(\"<-\", \"\\t\" * len(self.mstack), m)\n",
    "        self.the_vars.set_current_method(self.mstack[-1])\n",
    "\n",
    "    def on_call(self, arg, cxt, my_vars):\n",
    "        self.call_enter(cxt.method)\n",
    "        super().on_call(arg, cxt, my_vars)\n",
    "\n",
    "    def on_return(self, arg, cxt, my_vars):\n",
    "        super().on_return(arg, cxt, my_vars)\n",
    "        self.call_exit(cxt.method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar(traces):\n",
    "    m = Infer()\n",
    "    for inputstr, trace in traces:\n",
    "        st = TaintedTracker(inputstr, trace)\n",
    "        dt = Miner(inputstr, st.the_vars.defs)\n",
    "        m.add_tree(dt)\n",
    "    return m.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restrict = {'methods':['process_inventory', 'process_vehicle', 'process_car', 'process_van']}\n",
    "with TaintedTracer(INVENTORY, restrict) as tracer:\n",
    "    process_inventory(tracer())\n",
    "sm = TaintedTracker(tracer.inputstr, tracer.trace)\n",
    "#grammar = recover_grammar(traces)\n",
    "for k, v in sm.the_vars.defs.items():\n",
    "    print(\"%s = <%s> \\t %s\" % (k,v, len(v.taint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedMiner(Miner):\n",
    "    def get_derivation_tree(self):\n",
    "        my_assignments = self.my_assignments.copy()\n",
    "        root = (START_SYMBOL[1:-1], my_assignments[START_SYMBOL], [])\n",
    "        del my_assignments[START_SYMBOL]\n",
    "        for k, v in my_assignments.items():\n",
    "            self.insert_into_tree(root, (k, v, []))\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedMiner(TaintedMiner):\n",
    "    def insert_into_tree(self, root, elt):\n",
    "        # for each children of root, see if we are a\n",
    "        # subset of taints. If none, add ourself as a first level childA\n",
    "\n",
    "        ek, ev, eitems = elt\n",
    "        first_level = True\n",
    "        # first, look through the children to figure out if any one is a super set.\n",
    "        for child in root[2]:\n",
    "            if ev.taint_in(child[1]):\n",
    "                assert first_level\n",
    "                first_level = False\n",
    "                self.insert_into_tree(child, elt)\n",
    "\n",
    "        if not first_level:\n",
    "            return root\n",
    "\n",
    "        # then look through the children to figure out *how many* of them are subsets.\n",
    "        children = list(root[2])\n",
    "        new_children = []\n",
    "        while children:\n",
    "            child, *children = children\n",
    "            if child[1].taint_in(ev):\n",
    "                first_level = False\n",
    "                self.insert_into_tree(elt, child)\n",
    "            else:\n",
    "                new_children.append(child)\n",
    "\n",
    "        if not first_level:\n",
    "            new_children.append(elt)\n",
    "            root[2][:] = new_children\n",
    "            return\n",
    "\n",
    "        # neither superset or subset. Time to just append.\n",
    "        # check for overlap.\n",
    "        children = root[2]\n",
    "        for c in children:\n",
    "            for t in c[1].taint:\n",
    "                if t in ev.taint:\n",
    "                    assert False\n",
    "        root[2].append((ek, ev, eitems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that all children are in the right positions. In particular, assert that there are no overlaps. (If there are overlaps, we might have to choose between one of the overlapped elements based on the height of the tree of that element. -- Remember that each element is inserted as a child to *all* matching elements, and not just the first matching one. Hence, we can afford to choose between the trees and not worry about transferring elements across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(tree):\n",
    "    def simplify(var):\n",
    "        return re.sub(r'\\[.+\\]', '', var)\n",
    "    taint_start, taint_stop = tree[1].taint[0], tree[1].taint[-1]\n",
    "\n",
    "    new_children = []\n",
    "    prev_child = None\n",
    "    for child in sorted(tree[2], key=lambda c: c[1].taint[0]):\n",
    "        if prev_child:\n",
    "            assert prev_child[1].taint[0] < child[1].taint[0]\n",
    "        prev_child = child\n",
    "        child_t_start, child_t_stop = child[1].taint[0], child[1].taint[-1]\n",
    "        if child_t_start > taint_start:\n",
    "            elt = ''.join(tree[1].x(slice(taint_start, child_t_start)))\n",
    "            new_children.append((\"<%s>\" % repr(elt), [(repr(elt), [])]))\n",
    "        new_children.append(to_tree(child))\n",
    "        taint_start = child_t_stop + 1\n",
    "    if taint_start < taint_stop+1:\n",
    "        elt = ''.join(tree[1].x(slice(taint_start, taint_stop + 1)))\n",
    "        new_children.append((elt, []))\n",
    "    return (\"<%s>\" % simplify(tree[0]), new_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restrict = {'methods':['process_inventory', 'process_vehicle', 'process_car', 'process_van']}\n",
    "with TaintedTracer(INVENTORY, restrict) as tracer:\n",
    "    process_inventory(tracer())\n",
    "sm = TaintedTracker(tracer.inputstr, tracer.trace, track_return=True)\n",
    "dt = TaintedMiner(tracer.inputstr, sm.the_vars.defs)\n",
    "display_tree(to_tree(dt.tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_num(s,i):\n",
    "    n = ''\n",
    "    while s[i:] and s[i].in_(list(string.digits)):\n",
    "        n += s[i]\n",
    "        i = i +1\n",
    "    return i,n\n",
    "\n",
    "def parse_paren(s, i):\n",
    "    assert s[i] == '('\n",
    "    i, v = parse_expr(s, i+1)\n",
    "    if s[i:] == '':\n",
    "        raise Exception(s, i)\n",
    "    assert s[i] == ')'\n",
    "    return i+1, v\n",
    "\n",
    "\n",
    "def parse_expr(s, i = 0):\n",
    "    expr = []\n",
    "    while s[i:]:\n",
    "        c = s[i]\n",
    "        if c.in_(list(string.digits)):\n",
    "            i,num = parse_num(s,i)\n",
    "            expr.append(num)\n",
    "        elif c.in_(['+', '-', '*', '/']):\n",
    "            expr.append(c)\n",
    "            i = i + 1\n",
    "        elif c == '(':\n",
    "            i, cexpr = parse_paren(s, i)\n",
    "            expr.append(cexpr)\n",
    "        elif c == ')':\n",
    "            return i, expr\n",
    "        else:\n",
    "            raise Exception(s,i)\n",
    "    return i, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPRS = [\n",
    "    '12+23',\n",
    "    '(25+1)+100+(33+2)+1',\n",
    "    '(25-1/(2+3))*100/3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parse_expr(xtstr(EXPRS[1]).with_comparisons([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(tree):\n",
    "    key, val, children = tree\n",
    "    taint_start, taint_stop = val.taint[0], val.taint[-1]\n",
    "\n",
    "    new_children = []\n",
    "    for child in sorted(children, key=lambda c: c[1].taint[0]):\n",
    "        ckey, cval, cchildren = child\n",
    "        child_t_start, child_t_stop = cval.taint[0], cval.taint[-1]\n",
    "        # if child taint starts after the root taint, then we have a child\n",
    "        # missing with index from taint_start to child_taint_start(non-incl)\n",
    "        if child_t_start > taint_start:\n",
    "            elt = ''.join(val.x(slice(taint_start, child_t_start)))\n",
    "            new_children.append((\"<%s>\" % repr(elt), [(repr(elt), [])]))\n",
    "        # and we append child as before.\n",
    "        new_children.append(to_tree(child))\n",
    "        # the new taint_start begins next to the taint stop.\n",
    "        taint_start = child_t_stop + 1\n",
    "    if taint_start < taint_stop + 1:\n",
    "        elt = ''.join(val.x(slice(taint_start, taint_stop + 1)))\n",
    "        new_children.append((elt, []))\n",
    "    return (\"<%s>\" % key, new_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(EXPRS[1])\n",
    "restrict = {'methods':['parse_expr', 'parse_paren', 'parse_num']}\n",
    "with TaintedTracer(EXPRS[1], restrict) as tracer:\n",
    "    parse_expr(tracer())\n",
    "sm = TaintedTracker(tracer.inputstr, tracer.trace)\n",
    "dt = TaintedMiner(tracer.inputstr, sm.the_vars.defs)\n",
    "display_tree(to_tree(dt.tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_trees(trace, inputstr):\n",
    "    # name,stack,children,idxs\n",
    "    def new_node(s, stack, mid=None, indexes=None):\n",
    "        n = {\n",
    "            'sym': '<%s>' % s,\n",
    "            'id': mid,\n",
    "            'stack': stack,\n",
    "            'indexes':  [],#[inputstr[i] for i in indexes] if indexes is not None else [],\n",
    "            'children': []\n",
    "        }\n",
    "        for i in (indexes or []):\n",
    "            n_addindex(n, i)\n",
    "        return n\n",
    "\n",
    "    root = new_node('start', stack=0, mid=0)\n",
    "    method_stack = [root]\n",
    "\n",
    "    def n_addchild(node, n): node['children'].append(n)\n",
    "    def n_mid(node):      return node['id']\n",
    "    def n_sym(node):      return node['sym']\n",
    "    def n_addindex(node, i):\n",
    "        c = inputstr[i]\n",
    "        #node['indexes'].append(c)\n",
    "        if node['children']:\n",
    "            last_child = node['children'][-1]\n",
    "            if last_child.get('id') is None: # last child an index. Hence, append\n",
    "                last_child['indexes'].append(c)\n",
    "                last_child['sym'] = ''.join(last_child['indexes'])\n",
    "            else: # last child a method. Hence, start a new node\n",
    "                n_addchild(node, {'sym': c, 'indexes': [c]})\n",
    "        else: #no last_child. Start a new node\n",
    "            n_addchild(node, {'sym': c, 'indexes': [c]})\n",
    "\n",
    "    print(inputstr)\n",
    "    prev_idx = None\n",
    "    prev_node = None\n",
    "    last_cmp_only = []\n",
    "    for idx, instr, (method_name, stack_len, mid) in trace:\n",
    "        if idx == prev_idx:\n",
    "            prev_node = (idx, method_name, stack_len, mid)\n",
    "        else:\n",
    "            if prev_node:\n",
    "                last_cmp_only.append(prev_node)\n",
    "            prev_node = (idx, method_name, stack_len, mid)\n",
    "            prev_idx = idx\n",
    "    last_cmp_only.append(prev_node)\n",
    "    for idx, method_name, stack_len, mid in last_cmp_only:\n",
    "        print(\"%2d\" % idx, \" \", inputstr[idx], '\\t|' * stack_len, method_name,\n",
    "              mid, \"(%d)\" % stack_len)\n",
    "    print()\n",
    "    for idx, method_name, stack_len, mid in last_cmp_only:#[:4]:\n",
    "        last_idx = len(method_stack) -1\n",
    "        print(inputstr)\n",
    "        print(\"%2d\" % idx, \" chr:%s\" % inputstr[idx], '\\t|' * stack_len, \"\\t\",\n",
    "              method_name, mid, \"(%d)\" % stack_len)\n",
    "        if stack_len > last_idx:\n",
    "            print('###### append parent')\n",
    "            # first, generate a chain of the given stack length to the parent.\n",
    "            for i in range(len(method_stack), stack_len):\n",
    "                node = new_node('*',stack=i, mid=None)\n",
    "                n_addchild(method_stack[-1],node)\n",
    "                method_stack.append(node)\n",
    "\n",
    "            assert len(method_stack) == stack_len\n",
    "            # then for the given parent, add a child\n",
    "            node = new_node(method_name, stack=stack_len, mid=mid, indexes=[idx])\n",
    "            current = method_stack[-1]\n",
    "            n_addchild(current,node)\n",
    "            method_stack.append(node)\n",
    "            print(\"ROOT:\")\n",
    "            print(json.dumps(root, indent=2))\n",
    "            print('STACK:', [\"%d:%s\" % (i['stack'], i['sym']) for i in method_stack])\n",
    "        elif stack_len == last_idx:\n",
    "            current = method_stack[-1]\n",
    "            if n_mid(current) == mid:\n",
    "                print('###### insert char')\n",
    "                assert n_sym(current) == \"<%s>\" % method_name\n",
    "                current = method_stack[-1]\n",
    "                n_addindex(current, idx)\n",
    "            else:\n",
    "                print('###### start new peer node')\n",
    "                # a peer of this node. So get parent\n",
    "                method_stack.pop()\n",
    "                current = method_stack[-1]\n",
    "                node = new_node(method_name, stack=stack_len, mid=mid, indexes=[idx])\n",
    "                n_addchild(current,node)\n",
    "            print(\"ROOT:\")\n",
    "            print(json.dumps(root, indent=2))\n",
    "            print('STACK:', [\"%d:%s\" % (i['stack'], i['sym']) for i in method_stack])\n",
    "        elif stack_len < last_idx:\n",
    "            print('###### pop')\n",
    "            current = method_stack[-1]\n",
    "            print(\"-\", current)\n",
    "            while stack_len < last_idx:\n",
    "                print('<-')\n",
    "                method_stack.pop()\n",
    "                last_idx = len(method_stack)-1\n",
    "            assert stack_len == last_idx\n",
    "            current = method_stack[-1]\n",
    "            \n",
    "            print('###### insert char')\n",
    "            if n_mid(current) is None:\n",
    "                current['id'] = mid\n",
    "                current['sym'] = \"<%s>\" % method_name\n",
    "            else:\n",
    "                assert n_mid(current) == mid, \"%s != %s\" %(n_mid(current), mid)\n",
    "                assert n_sym(current) == \"<%s>\" % method_name\n",
    "            current = method_stack[-1]\n",
    "            n_addindex(current, idx)\n",
    "            \n",
    "            print(\"ROOT:\")\n",
    "            print(json.dumps(root, indent=2))\n",
    "            print('STACK:', [\"%d:%s\" % (i['stack'], i['sym']) for i in method_stack])\n",
    "            \n",
    "    print()\n",
    "    print(\"last:\", root)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_root = parse_trees(tracer.inputstr.comparisons, str(tracer.inputstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(node, my_str):\n",
    "    sym = node['sym']\n",
    "    children = node.get('children')\n",
    "    if children:\n",
    "        return (sym, [to_tree(c, my_str) for c in children])\n",
    "    else:\n",
    "        return (sym, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tree(my_root, str(tracer.inputstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tracer.inputstr)\n",
    "display_tree(to_tree(my_root, str(tracer.inputstr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* Given a set of inputs, we can learn an input grammar by examining variable values during execution.\n",
    "* The resulting grammars can be used right during fuzzing.\n",
    "* TODO: make the point that our initial implementation is about learning regular grammar not CFG because we do not know how to handle mutually recursive and looping procedures\n",
    "* TODO: Use process_vehicle as a pervading example.\n",
    "* TODO: Mention that control flow dependencies is not tracked in dynamic taints. But it is tracked in simple miner with string inclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\\cite{Lin2008}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "_Close the chapter with a few exercises such that people have things to do.  To make the solutions hidden (to be revealed by the user), have them start with_\n",
    "\n",
    "```markdown\n",
    "**Solution.**\n",
    "```\n",
    "\n",
    "_Your solution can then extend up to the next title (i.e., any markdown cell starting with `#`)._\n",
    "\n",
    "_Running `make metadata` will automatically add metadata to the cells such that the cells will be hidden by default, and can be uncovered by the user.  The button will be introduced above the solution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
